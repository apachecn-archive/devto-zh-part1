# (不是很)深度学习

> 原文：<https://dev.to/andrewlucker/not-very-deep-learning>

Tensorflow 不是独角兽，它只是另一个工具

在过去的一年里，我一直在通过 OpenAI 平台玩 ATARI 游戏，尝试不同的算法。这些游戏的好处在于所有的行动(或不行动)都是确定的。如果你做了同样的选择，那么你会得到同样的结果。

到目前为止，我发现性能最好的是 A3C 算法。A3C 类似于 DQN，但是它使用相同策略的许多类似变体来学习什么行为或见解对价值有很大影响，或者什么信息可以安全地忽略。这对于解决一些较简单的问题来说已经足够好了，但是它无法更深入地了解在对象识别方面有具体困难的复杂环境。

ATARI River Raid 环境就是一个很好的例子。这是我的一个 A3C 机器人在玩:

[https://www.youtube.com/embed/a4wxAWY-Dgk](https://www.youtube.com/embed/a4wxAWY-Dgk)

如果你观看这个片段，你会看到这个机器人采用了“避开障碍物并保持射击”的基本策略。如果不是因为游戏创造的“燃料”概念，这将是一个很好的策略。为了在等级上更进一步，有一个延迟的需求 1)不射击燃料盒和 2)通过触摸它们来收集它们。问题是忽略燃料费用没有短期的惩罚，因此代理人永远不会知道它们是重要的。这个机器人训练有素，所以我怀疑更多的训练对这个问题没有多大帮助。从根本上说，这是一个价值搜索广度与深度的问题。

我更希望在人工智能研究领域看到的是将对象和特征识别与策略强化学习相结合的方法。这种组合将有助于简化这些游戏环境，并有助于代理人削减每个环境中的巨大冗余。雅达利游戏不像围棋。每个像素都不是至关重要的。除了子弹和高射炮，通常物体都是较大的精灵。

这就是我现在的困境。我将尝试自己做一些对象识别的研究，看看我是否可以用策略学习方面来纠正这个研究。希望我们能很快看到更深的东西，目前的速度令人鼓舞。