# NaNoGenMo 2016 和我对文本生成的预测

> 原文：<https://dev.to/tra/nanogenmo-2016-and-my-predictions-about-text-generation>

# NaNoGenMo 2016

NaNoGenMo 代表国家小说世代月。你能写出一个月能生成 5 万字小说的代码吗？自 2013 年问世以来，它已经慢慢获得了以各种不同方式解决一个非常困难的问题(大规模生成文本)的创新声誉。

[NaNoGenMo 2016](https://github.com/NaNoGenMo/2016) 似乎不如 [NaNoGenMo 2015](https://github.com/dariusk/NaNoGenMo-2015) 或 [NaNoGenMo 2014](https://github.com/dariusk/NaNoGenMo-2014) 活跃。总的来说，它的问题少了，明星也少了。曾经广泛传播的媒体关注度已经显著下降——唯一值得注意的新闻文章是 [Nerdcore](http://www.nerdcore.de/2016/11/30/%E2%96%88-blackout-%E2%96%88-generative-censorship-poetry/) 承认生成小说[The Days Left prediction 和 Water](https://github.com/NaNoGenMo/2016/issues/119) 的存在。覆盖范围似乎仅限于同情的来源(通常来自其他程序员)。

然而，在不活跃的表面下，人们的态度似乎发生了巨大的变化。关于生成性文本的知识已经在博客文章中正式化了(比如我以前在 dev.to 上写过的文章，还有一篇俄语博客文章),以便于参考。更多的新手似乎渴望并充满激情地参与其中。如果学生[提交](https://github.com/NaNoGenMo/2016/issues/115)参赛作品，计算机科学导师会给他们的[额外的](https://github.com/NaNoGenMo/2016/issues/104) [学分](https://github.com/NaNoGenMo/2016/issues/150)。一门课要求[提交一本生成性书籍](https://github.com/NaNoGenMo/2016/issues/148)。电脑生成的散文现在可读性更强，也更有趣，尽管还需要做更多的工作。

文本生成领域是令人欣慰的，而且可能会变得更好。这就引出了一个问题...这个领域接下来会发生什么？回顾 2016 年 NaNoGenMo 上发生的事情，我愿意大胆地做 3 个预测...

# 统计方法将成为文本生成“工具箱”的一部分

在 NaNoGenMo 2016 期间，Vincent Toups 发布了一个 [Tweetstorm](https://twitter.com/i/moments/809240284664041474) 来检查文本生成中使用的两种主要方法——“统计”方法和“结构建模”方法。我非常关注“结构建模”...但是在“统计”方法(例如，机器学习)上就不那么重要了。

> [“统计”方法]试图通过数据集创建特定的生成模型。生成方法是广泛的:递归神经网络或词向量或马尔可夫模型是这种方法中使用的工具。

文森特·图普斯指出了与“统计”方法相关的两个问题:

> 给定小说的统计数据非常糟糕。大多数小说都包含一系列不同的句子。即使对于“不同”的松散定义。因此，如果你试图学习一个句子模型，你没有足够的数据。
> 
> ...天真模型的第二个主要问题是结构:小说不是句子的序列。它们编码了复杂、庞大的相关性。最值得注意的是，情节。但是风格、特征等等都存在于天真的模型难以描述的尺度上。

统计方法有利于呈现看起来像人类的文本。它能非常有效地产生引人入胜的单词、短语和句子。然而，生成的文本很快就会变成胡言乱语，即使是短时间的暴露。生成的文本无法“放大”成长篇小说。

我之前写的关于“文本生成”的博客文章涉及了文森特·图普斯所说的“结构建模”。建模的想法是程序员编写代码，对大规模的情节/风格/特征进行“编码”。即使这些模型也有它们的弱点和缺陷，但是它们确实比“统计”方法产生了更具可读性的文本。正如文森特指出的-

> [他们]向非创作的作品爬去，因为作者就在其中。

尽管 Vincent 指出了“统计”方法和“结构建模”方法的局限性，但他也表达了两者融合的希望:

> 我认为一个真正成功的尝试将包括混合的方法:故事和弧线的模型，风格的统计技术。即便如此，我怀疑结果会缺乏人情味。毕竟，文学是人类之间复杂的交流行为。

2016 年，一些程序员已经开始转向这种混合方法，使用“统计方法”和“结构建模”来生成小说:

*   Isaac Karth 使用了用于高级“自然语言处理”的 Python 模块 [Textacy](https://github.com/chartbeat-labs/textacy) ，在古腾堡语料库中搜索可以组合成模板的关键句子。这些模板将被用来为他的[海盗小说](https://github.com/NaNoGenMo/2016/issues/6)生成场景。然而，艾萨克承认他也用 grep 来寻找关键句子。

*   super threak 使用相似的句子来制造“职场牢骚”...从语料库中随机选择一个句子，以及几个相似的句子。这就产生了一个连贯的微型句子，因为微型句子中的每个句子都是相互关联的。

*   启动纸浆厂！生成以幻想宇宙为背景的短篇故事。该算法使用马尔可夫链来生成角色可以探索的各个位置的名称，并使用模板和战斗模拟来解释角色在这些位置实际上*做了什么*。

*   《年鉴》是一本关于一个大国历史的小说。[它使用神经网络](http://mikelynch.org/2016/Dec/10/annales-1-vocab/)生成“不仅仅是名字，还有生物、形容词、抽象名词、武器、食物和饮料等等”。生成的单词被过滤以匹配某些标准(例如，所有女性的名字必须以字母“a”结尾)。根据迈克·林奇的说法，使用神经网络来生成单词“使输出更加神秘，并摆脱了早期草稿中许多可笑的时代错误”。

这种“混合方法”很可能在未来几年里取得丰硕成果。通过利用建模和统计技术，程序员将获得“两全其美”。

# 语料库管理将是“文本生成”中的一项必要技能

一个人能在对他的世界一无所知的情况下写一个故事吗？没有概念，想法，甚至语言？我怀疑这是可能的。人类需要社会输入才能产生有意义的输出。机器也需要语料库形式的输入。

当前对人工智能的大肆宣传与大数据的兴起不谋而合。机器能够比人更有效地处理和操纵数据，从而使它们适合于生成关于世界的有用的统计预测和分析(即使有人承认机器并不真正“理解”世界)。

数据的文本生成对等物是“语料库”，即输入计算机的书面文本。计算机可以将语料库中的文本拼接在一起，生成一个故事，就像人类使用一种语言的字母来生成一个故事一样。文本生成需要语料库，即使语料库和[模板](https://dev.to/tra)一样基础。甚至像 RNNs 这样的[机器学习算法也需要某种预先存在的语料库来进行训练。一个程序需要一个语料库来有效地写作(就像人类需要预先存在的知识来有效地写作一样)。](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

结果是程序员也必须从事“语料库管理”。程序员必须搜索可以被算法用来生成故事的文本。寻找语料库的过程可能会很困难且耗时，因此许多 NaNoGenMo 程序员依赖 [Gutenebrg](http://www.gutenberg.org) 和[语料库](https://github.com/dariusk/corpora)来获取他们的文本。如果特定的语料库不容易获得，那么程序员必须手写语料库，这是一个相当痛苦的过程。

通常，随着语料库的增长，生成的输出会变得更加多样和有趣。这意味着改进文本生成器的最好方法就是收集更多的数据。当然，收集数据的过程本身就是一项昂贵的努力，迫使程序员在如何使用他们已经拥有的语料库方面更具创造性。

来自 2016 NaNoGenMo 的示例:

*   在这部小说中，主角潜入垃圾箱寻找关于他的世界的有价值的剪报和技术手册。每当垃圾箱搜索成功时，就会向用户显示新的语料库。

*   追踪法把古腾堡的段落一段接一段地放在一起，生成了一部人类可读的 13606 字的中篇小说，讲述了一个生活在地下综合体的特工。

*   《如果在冬天的夜晚，一个图书馆持卡人》讲述了一个人试图从古腾堡文集里找到某本书的故事。每当主角打开一本书，你会看到从那本书里直接随机选取的一段，然后主角决定是保留这本书还是把它放回书架。

延伸阅读:

*   [该文集的制备应算作纳米基因的新制备吗？](https://groups.google.com/forum/#!topic/generativetext/TiigaPYxe7U)讨论语料库在 NaNoGenMo 中的作用，并解释为什么语料库重用对程序员有用。

# 程序员的目标将是‘规模化’新奇体验，*而不是*省钱

关于自动化的传统说法是，机器能够做与人类相同的工作，但成本要低得多。结果，人类很可能会失业，因为自动化生产效率更高。

虽然我同意大多数行业的这种传统说法，但将其应用于生成性文本有点困难。作家的工资本来就很低(如果算上博客志愿者和社交媒体用户，工资就不存在了)。从廉价的人工劳动力转向廉价的自动化劳动力所带来的任何效率提升都是有限的...那是如果你忽略初始安装成本的话。成本节约可能以某种形式存在，但这不会是人们使用电脑写作的原因。

相反，计算机具有“放大”有趣想法的优势。一个人可能会拒绝在一瞬间为一千个人写个性化的十四行诗，为赛博朋克视频游戏中的数百个不同角色生成对话树，或者写十二个可怕的詹姆斯·邦德双关语。但是电脑不在乎。[代码只是形式化过程的一种方式](https://dev.to/tra/defining-the-industry)，计算机使程序员能够将创造性的想法转化为有形的最终产品。

不过，计算机能在多大程度上“放大”这些有趣的想法是有限度的。非常容易产生无限变异，但是人类擅长模式匹配，会很容易发现你们这一代的模式。一旦人类发现了这些模式，他们可能会感到厌倦。人类文学也有模式，但机器的模式可能更简单，更容易识别和确定。

聪明的程序员可以通过开发更有趣的模式，创建多种模式，或者增加机器可以利用的语料库来解决这个限制...但是这需要和手工制作内容一样多的工作。这就是为什么 Cookie Clicker 的开发者 Orteil[发了一条推特](https://twitter.com/Orteil42/status/802258188498333701):

> 多亏了程序生成，我可以在两倍的时间内生成两倍的内容

尽管如此，计算机可以讲述人类以前从未想过要讲述的故事。通过提供新的有趣的体验，计算机生成的文学也许能够获得一个有价值的位置。这可能是它所能希望的一切，因为机器生成的文学成为它自己独特的流派，拥有自己专门的粉丝群(尽管大多数人对人类生成的作品很满意)。但也有可能，新奇的吸引力可能会使机器生成的文学达到主流地位。

来自 2016 NaNoGenMo 的示例:

*   《不祥的日子》和《水》从事“黑色诗歌”，在文本中突出某些单词，而所有其他单词都被涂黑。

*   追逐——一部偏执的惊悚小说是一部关于越来越疯狂的追逐场景的小说，其中有政治掩盖、阴暗的
    会议和不安全的聊天室。

*   亲爱的圣诞老人使用推特来写关于人们欲望的“当代书信体小说”。

*   《贡特诗学入门》是一本关于人类试图翻译和理解外星诗歌的学术教科书。

延伸阅读:

*   谁是电脑合成小说的观众？解释了计算机生成的文学作品可以向读者传递的不同类型的体验。

# 附录

这篇博文是计算机生成的，使用了手写的语料库。[源代码在这里](https://gist.github.com/tra38/d680b9cfd4eab531fde3fa4691fa9d50)。我使用 Ruby gem [Prolefeed](https://github.com/tra38/Prolefeed) 生成这篇博文。

# 以前的文章以文字代

*   [电脑生成小说中的结构](https://dev.to/tra/structure-in-computer-generated-novels)

*   [在计算机生成作品中使用模板](https://dev.to/tra/using-templates-innbspcomputer-generated-works)

*   [计算机生成作品中的“常识”问题](https://dev.to/tra/the-commonsense-knowledge-problem-in-computer-generated-works)

*   谁是电脑合成小说的观众？