# TL；DR: Dynamo:亚马逊的高可用性键值存储

> 原文:[https://dev . to/Elvin yung/tldr-dynamo-amazons-high-available-key-value-store](https://dev.to/elvinyung/tldr-dynamo-amazons-highly-available-key-value-store)

*交叉发布自[我的个人博客](http://blog.elvinyung.com/post/tldr-dynamo/)；这是我试图让 systems-y 论文更容易理解的一系列博客文章的一部分。*

在 [SOSP 2007](http://www.sosp2007.org/) 展会上，亚马逊展示了 *[Dynamo:亚马逊高度可用的键值存储](https://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf)* 。这个东西“启动了一千个 NoSQL 数据库”——连同谷歌的 Bigtable，它开启了 2000 年代末和 2010 年代初的 NoSQL 运动。

[上一次](//../tldr-chubby)，我们看了 Google 的 Chubby，它提供了一个使用 Paxos 复制的键值存储，以实现强一致性。在很多方面，迪纳摩是小胖的反义词。

迪纳摩——不要与迪纳摩 ***DB*** 混淆，后者的灵感来自迪纳摩的设计——是一个键值存储，旨在实现可伸缩性和高可用性。用 [CAP 定理](https://jvns.ca/blog/2016/10/21/consistency-vs-availability/)的术语来说，Chubby 是一个 CP 系统，而 Dynamo 则在另一端，正好属于 AP 系统的范畴。

驱动迪纳摩设计原则的核心基本原理是观察到系统的*可用性*与所服务的客户数量直接相关。另一方面，不完美的*一致性*通常可以被屏蔽，并在客户不知情的情况下在后端解决。受这一主要思想的影响，Dynamo 积极地优化可用性，并做了一些非常有趣的权衡。

发电机的设计非常有影响力。它启发了大量的 NoSQL 数据库(有时被归为*发电机系统*的类别)，如[卡珊德拉](https://cassandra.apache.org/)、[里亚克](http://basho.com/products/)和[伏地魔](http://www.project-voldemort.com/voldemort/)——更不用说亚马逊自己的[发电机数据库](https://aws.amazon.com/dynamodb/)。核心架构甚至影响了像 Ringpop(一个负载均衡器)和 LeoFS(一个分布式文件系统)这样的项目。

在这篇文章中，我将花一些时间谈论 Dynamo 的核心设计(偶尔会交叉引用一些开源的 Dynamo 系统)，以及它是如何影响软件行业的。

(通常的免责声明是，我将把重点放在使事情易于理解，而不是严格正确。)

## [](#design-goals)设计目标

正如我上面提到的，Dynamo 的目标是，高于一切，是高度可用的。总之，他们提出了这些基本特性:

*   系统应该是**可增量扩展的**。您应该能够将一台机器放入系统中，并看到相应的改进。
*   不应该有**领导**流程。领导者是一个单点故障，它在某个点上成为系统的瓶颈，使系统更难扩展。如果每个节点都是一样的，复杂性就消失了。
*   数据应该被乐观地复制，这意味着不需要花费写时间来确保整个系统的正确性，不一致性应该在其他时间解决。

最终，迪纳摩围绕六项核心技术构建:

*   **一致散列**在节点之间分片数据，并使添加新节点变得容易。
*   一个**八卦协议**，用于跟踪集群状态，并通过使用**提示切换**使系统“始终可写”。
*   将写入复制到系统中其他节点的**松散仲裁**中，而不是像 Paxos 那样的严格多数仲裁。
*   由于无法保证写入时节点的值一致，因此请使用其他机制来解决潜在的冲突:
    *   使用**向量时钟**跟踪值历史，并在读取时协调不同的历史。
    *   在后台，使用 **Merkle trees** 解决不同节点之间的冲突。

## [](#data-model)数据模型

为了提高可伸缩性，Dynamo 只公开了一个非常简单的接口:你可以`get()`一个键，也可以`put()`一个键。值被视为不透明斑点。换句话说，Dynamo 提供了巨大的可伸缩性，仅此而已。

一个简单的键-值数据模型是很棒的，因为它是令人尴尬的并行:因为一个单键操作除了它的值之外不依赖任何东西，所以你可以很容易地将工作负载划分到任意多个进程中。但这与它所取代的传统关系数据库相去甚远，大多数人都习惯于传统关系数据库。

在标准的 RDBMS 中，你有一堆元组，它们被组织成一堆关系。除此之外，您还可以获得外键、连接、丰富的查询语言和保证正确性的事务等特性。但是在一个可能分布在多个大陆的多台机器上的系统中实现这些特性是非常昂贵的，尤其是在十多年前 Dynamo 刚被开发出来的时候。

其结果是，尽管 Dynamo 应该取代关系数据库，但它实际上是将这些责任推给了应用程序。使用 Dynamo 系统，如果应用程序开发人员需要确保他们的数据被正确地写入系统，或者正确地引用事物，他们必须编写自己的逻辑来做到这一点。

后续的迪纳摩系统在这方面稍微好一点；Cassandra，Riak 和 DynamoDB 都有实型系统。(它们仍然缺少外键之类的东西，但这更容易理解，因为跨分布式系统的参照完整性真的很慢。)

## [](#consistent-hashing)一致散列

您可能已经知道哈希表是如何工作的:您有一个桶数组；要想知道一个键属于哪个桶，可以通过一个散列函数传递它，结果以表的大小为模就是您想要的桶的索引。散列是一种将任意键映射到有限范围的好方法。

但是，当您需要调整数据结构大小时，有时需要重新构建整个数据结构。这对于小型内存哈希表来说没问题，但是对于像 Dynamo 这样的数 TB 存储来说，这是灾难性的，可能意味着数小时的停机时间。

一致散列法是解决这个问题的一种方法。其思想是索引和存储桶之间不是一对一的映射，而是一个连续的索引范围映射到一个存储桶。每当您想要添加一个存储桶时，您只需分割一个现有的索引范围。

整个范围被视为一个*环*，在最大索引后环绕。每个桶在环上被分配一个随机点。为了找出一个密钥应该进入哪个桶，你散列你的密钥来找出索引，然后你*沿着环顺时针*走，直到你到达一个桶——那个桶就是密钥应该去的地方。

(敏锐的读者会注意到，简单实现的最坏情况性能是当您将单个存储桶分配给最大可能的索引时，您必须遍历整个环才能到达它。在现实生活中，哈希环通常被实现为平衡二叉树，以避免这个问题。)

太棒了，这意味着一致的散列可以增量地完成*:一个新的桶会像往常一样被随机放置到环上，这意味着它会接管一些现有桶的一部分。如果有`k`键和`n`桶，这意味着平均来说你只需要围绕`k/n`键移动。*

 *### [](#virtual-nodes)虚拟节点

我们刚刚看到的朴素一致性哈希算法有两个主要问题，它们都与密钥空间的分割方式有关。首先，即使我们期望均匀分布，我们可能也不会看到非常均匀的分布[，除非我们有很多节点](https://en.wikipedia.org/wiki/Law_of_large_numbers)。第二，我们无法控制一台机器存储多少数据:一台强大的机器可能与一台较弱的机器获得相同的密钥“份额”。伪随机数发生器是一个反复无常的东西。

迪纳摩很好地处理了这两个问题。一台物理机器实际上不仅仅是一个节点；它被视为多个*虚拟*节点，可以基于每台机器进行配置。这使得键更有可能均匀分布，并且具有更多资源的机器可以容易地被配置为存储更多数据，只需增加分配给它的虚拟节点的数量。相当聪明。

这篇论文实际上并没有谈到这一点，但是一致散列的一个问题是它没有明确地处理*热点*。如果一个特定的范围比其他范围被更频繁地访问，那么就没有办法分割这个范围。你添加更多的机器，并希望最好的。一篇卡珊德拉[的博客文章](http://www.datastax.com/dev/blog/we-shall-have-order)建议谨慎选择你的分片密钥。

## [](#replication)复制

为了复制更新，Dynamo 使用了一种叫做**松散定额**的东西。这与类似于 [Paxos](//../tldr-chubby) 的东西形成对比，后者依靠严格的法定人数共识来取得进展。Dynamo 允许您配置在客户端收到响应之前需要确认读或写的节点数量，而不是绝对多数。这意味着您不一定能获得 Paxos 提供的严格保证，但您可以获得更好的延迟和可用性。

主要的问题是，由于松散的仲裁并不是严格的多数，您的数据可能并且将会*发散*:对同一个键的两个并发写入有可能被不重叠的节点集接受。迪纳摩允许这样做，并在其他时间解决这些冲突。稍后将详细介绍。

提高可用性的一个有趣技巧是**暗示切换**:当一个节点不可达时，另一个节点可以代表它接受写操作。然后，写操作被保存在本地缓冲区中，并在目标节点再次可达时发送出去。这就是为什么迪纳摩*总是可写的*:即使在只有一个节点活跃的极端情况下，写请求仍然会被接受，并最终得到处理。

## [](#gossip-protocol)八卦协议

假设您有一个需要协同工作的节点集群。一个节点关闭。其他人是怎么发现的？(记住，我们没有一个可以充当真理来源的领袖节点。)

最简单的方法是让每个节点与其他节点保持[心跳](https://en.wikipedia.org/wiki/Heartbeat_(computing))。当一个节点出现故障时，它将停止发送心跳，其他所有人都会立即发现。但是随后`O(n^2)`消息会在每一个时钟周期被发送——这是一个高得离谱的数量，显然在任何大规模的集群中都不可行。

相反，Dynamo 使用了一个**八卦协议**。每个节点跟踪它认为集群看起来像什么，例如，哪些节点是可到达的，它们负责什么键范围，等等。(这基本上是哈希环的副本。)每个滴答，一个节点试图随机联系一个其它节点。如果另一个节点是活动的，那么这两个节点交换信息，并且两个节点现在看到相同的状态。

这意味着任何新事件最终都会在系统中传播。如果集群不再有变化，那么可以保证系统最终会收敛到相同的状态。

## [](#conflicts)冲突

这是事情变得令人兴奋的地方。松散的仲裁意味着系统中可能存在同一个键的多个冲突值，并且必须以某种方式解决。迪纳摩用的招数很少。

### [](#vector-clocks)矢量时钟

时间是一件棘手的事情。

在一台机器上，您需要知道的只是绝对时间或**挂钟**时间:假设您对时间戳为`t1`的键`k`执行一次写操作，然后对时间戳为`t2`的`k`执行另一次写操作。由于`t2 > t1`，第二次写入肯定比第一次写入更新，因此数据库可以安全地覆盖原始值。

在分布式系统中，这个假设不成立。问题是**时钟偏移**——不同的时钟往往以不同的速率运行，所以你不能假设节点`a`上的时间`t`发生在节点`b`上的时间`t + 1`之前。最实用的帮助同步时钟的技术，如 [NTP](https://en.wikipedia.org/wiki/Network_Time_Protocol) ，仍然不能保证分布式系统中的每个时钟都是同步的。因此，如果没有像 GPS 单元和原子钟这样的特殊硬件，仅仅使用挂钟时间戳是不够的。

因此，在没有严格同步的情况下，Dynamo 使用一种叫做**矢量时钟**的东西。基本上，对象基于因果关系的知识被赋予了一个*版本*(**发生在关系**之前)。可能会出现差异，并在读取时解决，或者由服务器自动解决，或者由客户端手动解决。一个非常简单的例子:

1.  节点`A`服务于对键`k`的写，值为`foo`。它给它分配一个版本的(`A`，1)。该写入被复制到节点`B`。
2.  出现网络分区。`A`和`B`不能互相通话。
3.  节点`A`服务于对键`k`的写，值为`bar`。它给它分配一个版本的(【T3，2】)。它不能将它复制到节点`B`，但是它被存储在某处的提示切换缓冲区中。
4.  节点`B`看到对键`k`的写，值为`baz`。它给它分配一个版本的(【T3，2】)。它不能将它复制到节点`A`，但是它被存储在某处的提示切换缓冲区中。
5.  节点`A`服务于对键`k`的读取。它看到(`A`，1)和(`A`，2)，但是它可以*自动*解决这个问题，因为它知道(`A`，2)是更新的，所以它返回`bar`。
6.  网络治愈。节点`A`和`B`可以再次相互通话。
7.  任一节点看到对关键字`k`的读取。它看到相同的密钥有不同的版本(`A`，2)和(`B`，2)，但是它*不知道哪个版本更新。它返回这两个版本，并告诉客户机自己找出答案，然后把新版本写回系统。*

### [](#merkle-trees)多积树木

如果一个范围的副本明显落后于其他副本，仅使用向量时钟可能需要很长时间来解决冲突。如果能在后台自动解决一些冲突就好了。要做到这一点，我们需要能够快速比较一个范围的两个副本，并准确地找出哪些部分是不同的。

一个范围可以包含大量数据。天真地分割校验和的整个范围不是很不可行；要传输的数据实在太多了。

相反，Dynamo 使用 **Merkle 树**来比较一个范围的副本。Merkle 树是一个二进制散列树，其中每个内部节点是其两个子节点的散列，每个叶节点是原始数据的一部分的散列。比较 Merkle 树在概念上很简单:

1.  比较两棵树的根哈希。
2.  如果相等，停止。
3.  在左右孩子身上递归。

最终，这意味着复制品确切地知道范围的哪些部分是不同的，但是交换的数据量被最小化。

### crdt

所有这些复杂的冲突解决逻辑，事实证明有一个更简单的方法来解决冲突:不要有冲突。使用**无冲突复制数据类型**。

主要思想是这样的:如果你可以将数据建模为一系列*可交换的*变化，即它们可以以*任何*顺序应用并具有相同的结果，那么你在系统中不需要任何排序保证。

购物车是一个很好的例子:添加一个商品`A`然后添加一个商品`B`可以从任何节点以任何顺序完成。(从购物车中删除被建模为负添加。)接收到同一组更新的任何两个节点将看到相同状态的想法被称为**强最终一致性**。

Riak 有一些[内置的 crdt](http://docs.basho.com/riak/kv/2.2.0/developing/data-types/)。

### [](#last-write-wins)最后写赢

不幸的是，CRDTs 并不像“只加交换运算”那么简单。实际上，除了非常简单的数据结构，通常很难将数据建模为 CRDT。在许多情况下，这样做太费力了，客户端解决方案已经足够好了。

其实很多时候，甚至更糟。因为仍然很难对向量时钟进行推理，所以 Dynamo 系统通常会在服务器端提供自动解决这些冲突的方法。Riak 和 Cassandra 部署通常使用一个简单的策略:基于挂钟时间戳，最后一次写入获胜。

还记得我上面提到的所有关于时钟偏移的事情吗？😱

挂钟 LWW 是一个丢失数据的好方法。如果冲突写在大约同一时间发生，你基本上是在*扔硬币决定写哪一个。[多了](https://aphyr.com/posts/299-the-trouble-with-timestamps) [多了](https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7#.5uonqtiyc) [多了](https://issues.apache.org/jira/browse/CASSANDRA-580) [说](https://aphyr.com/posts/285-jepsen-riak) [关于](http://queue.acm.org/detail.cfm?id=2610533) [这个](http://basho.com/posts/technical/clocks-are-bad-or-welcome-to-distributed-systems/)。*

## [](#conclusion)结论

我基本上跳过了我认为不必要的东西，但是我希望我已经给出了关于 Dynamo(以及受它启发的系统)如何工作的一个体面的概述。

迪纳摩确实是一项杰出的工程。以亚马逊的规模，迪纳摩的高可用性优化可能使其成为基础设施的关键部分。但同时，为了实现其可伸缩性和可用性目标，它需要在数据模型、特性以及更重要的安全性方面做出重大牺牲。

尽管像发电机一样的 NoSQL 系统一度准备接管(数据库的)世界，但钟摆又摆了回来。越来越多的工程团队正在走“更安全”的道路，即共享 PostgreSQL 或 MySQL 之类的关系数据库。此外，像 [F1](https://research.google.com/pubs/pub41344.html) 和 [H-Store](http://hstore.cs.brown.edu/) / [VoltDB](https://www.voltdb.com/overviews) 这样的“NewSQL”系统的成功表明，可伸缩性和强一致性是一个错误的二分法——事实上，同时拥有 NoSQL 的可伸缩性和关系数据库的特性和 ACID 正确性是可能的。我希望在以后的帖子里多谈谈他们。*